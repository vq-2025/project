{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VQ-GAN vs Efficient-VQGAN: ì™„ì „ ìë™ íŒŒì´í”„ë¼ì¸\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ í•˜ë‚˜ë¡œ ë‹¤ìŒ ëª¨ë“  ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:\n",
    "1. âœ… ë°ì´í„° ì¤€ë¹„\n",
    "2. ğŸ”¥ ëª¨ë¸ í•™ìŠµ (VQ-GAN + Efficient-VQGAN)\n",
    "3. ğŸ“Š ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬\n",
    "4. ğŸ“ˆ ë¹„êµ ì‹œê°í™”\n",
    "5. ğŸ“„ ë¦¬í¬íŠ¸ ìƒì„±\n",
    "\n",
    "**ì‹¤í–‰ ë°©ë²•:** ìœ„ì—ì„œë¶€í„° ìˆœì„œëŒ€ë¡œ ì…€ì„ ì‹¤í–‰í•˜ì„¸ìš”!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì • ë° Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ í™•ì¸\n",
    "import sys\n",
    "!{sys.executable} -m pip install -q torch torchvision tqdm matplotlib seaborn pandas pillow einops\n",
    "\n",
    "print(\"âœ“ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ëª¨ë“  í•„ìš”í•œ ëª¨ë“ˆ\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.utils import make_grid, save_image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML, Image as IPImage\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ìŠ¤íƒ€ì¼ ì„¤ì •\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8-darkgrid')\n",
    "except:\n",
    "    plt.style.use('seaborn-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"âœ“ Import ì™„ë£Œ!\")\n",
    "print(f\"PyTorch ë²„ì „: {torch.__version__}\")\n",
    "print(f\"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ import\n",
    "from vqgan.model import VQGAN, Discriminator as VQGANDiscriminator\n",
    "from efficient_vqgan.model import EfficientVQGAN, MultiScaleDiscriminator\n",
    "from utils.logger import TrainingLogger\n",
    "from utils.metrics import calculate_psnr, calculate_ssim\n",
    "\n",
    "print(\"âœ“ ëª¨ë¸ ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ì„¤ì • (ì—¬ê¸°ì„œ í•™ìŠµ íŒŒë¼ë¯¸í„° ì¡°ì •)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== í•™ìŠµ ì„¤ì • ====================\n",
    "CONFIG = {\n",
    "    # ê¸°ë³¸ ì„¤ì •\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'num_epochs': 5,  # ë¹ ë¥¸ ë°ëª¨ë¥¼ ìœ„í•´ 5 epoch (ì‹¤ì œ: 50-100)\n",
    "    'image_size': 256,\n",
    "    \n",
    "    # ë°ì´í„°\n",
    "    'dataset': 'cifar10',\n",
    "    'batch_size_vqgan': 4,\n",
    "    'batch_size_efficient': 8,\n",
    "    'num_workers': 0,  # macOS/Jupyterì—ì„œ ì•ˆì •ì„±ì„ ìœ„í•´ 0ìœ¼ë¡œ ì„¤ì •\n",
    "    \n",
    "    # í•™ìŠµ\n",
    "    'lr': 4.5e-6,\n",
    "    'disc_start': 500,  # Discriminator ì‹œì‘ step\n",
    "    'disc_weight': 0.8,\n",
    "    'perceptual_weight': 1.0,\n",
    "    \n",
    "    # ë¡œê¹…\n",
    "    'log_interval': 100,  # ëª‡ stepë§ˆë‹¤ ë¡œê·¸\n",
    "    'save_interval': 1,   # ëª‡ epochë§ˆë‹¤ ì €ì¥\n",
    "}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"í•™ìŠµ ì„¤ì •:\")\n",
    "print(\"=\"*60)\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key:25s}: {value}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ë°ì´í„° ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ë¡œë”©\n",
    "print(\"ë°ì´í„° ë¡œë”© ì¤‘...\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(CONFIG['image_size']),\n",
    "    transforms.CenterCrop(CONFIG['image_size']),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "val_dataset = datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "print(f\"âœ“ í•™ìŠµ ë°ì´í„°: {len(train_dataset)} ì´ë¯¸ì§€\")\n",
    "print(f\"âœ“ ê²€ì¦ ë°ì´í„°: {len(val_dataset)} ì´ë¯¸ì§€\")\n",
    "\n",
    "# ìƒ˜í”Œ ì´ë¯¸ì§€ í™•ì¸\n",
    "fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "for i in range(16):\n",
    "    img, _ = train_dataset[i]\n",
    "    img = (img + 1) / 2  # denormalize\n",
    "    axes[i//8, i%8].imshow(img.permute(1, 2, 0))\n",
    "    axes[i//8, i%8].axis('off')\n",
    "plt.suptitle('ìƒ˜í”Œ ì´ë¯¸ì§€', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. í—¬í¼ í•¨ìˆ˜ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceptual Loss\n",
    "class SimpleLPIPS(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        # Simple L2 perceptual loss (LPIPS ëŒ€ì²´)\n",
    "        return torch.mean((x - y) ** 2)\n",
    "\n",
    "def hinge_d_loss(logits_real, logits_fake):\n",
    "    loss_real = torch.mean(torch.relu(1. - logits_real))\n",
    "    loss_fake = torch.mean(torch.relu(1. + logits_fake))\n",
    "    return 0.5 * (loss_real + loss_fake)\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "def show_reconstruction(original, reconstructed, n=8):\n",
    "    \"\"\"ì›ë³¸ê³¼ ë³µì› ì´ë¯¸ì§€ ë¹„êµ\"\"\"\n",
    "    fig, axes = plt.subplots(2, n, figsize=(n*2, 4))\n",
    "    \n",
    "    for i in range(n):\n",
    "        # Original\n",
    "        img_orig = (original[i].cpu() + 1) / 2\n",
    "        axes[0, i].imshow(img_orig.permute(1, 2, 0).clamp(0, 1))\n",
    "        axes[0, i].axis('off')\n",
    "        if i == 0:\n",
    "            axes[0, i].set_title('Original', fontsize=10)\n",
    "        \n",
    "        # Reconstructed\n",
    "        img_recon = (reconstructed[i].cpu() + 1) / 2\n",
    "        axes[1, i].imshow(img_recon.permute(1, 2, 0).clamp(0, 1))\n",
    "        axes[1, i].axis('off')\n",
    "        if i == 0:\n",
    "            axes[1, i].set_title('Reconstructed', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"âœ“ í—¬í¼ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. VQ-GAN í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VQ-GAN í•™ìŠµ ì‹œì‘\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# ëª¨ë¸ ì´ˆê¸°í™”\n",
    "vqgan = VQGAN(\n",
    "    in_channels=3,\n",
    "    hidden_dims=[128, 256, 512],\n",
    "    latent_dim=256,\n",
    "    num_embeddings=1024\n",
    ").to(CONFIG['device'])\n",
    "\n",
    "discriminator_vq = VQGANDiscriminator(in_channels=3).to(CONFIG['device'])\n",
    "discriminator_vq.apply(weights_init)\n",
    "\n",
    "print(f\"VQ-GAN íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in vqgan.parameters())/1e6:.2f}M\")\n",
    "\n",
    "# ì˜µí‹°ë§ˆì´ì €\n",
    "opt_vq = optim.Adam(\n",
    "    list(vqgan.encoder.parameters()) + \n",
    "    list(vqgan.decoder.parameters()) + \n",
    "    list(vqgan.quantizer.parameters()),\n",
    "    lr=CONFIG['lr'], betas=(0.5, 0.9)\n",
    ")\n",
    "\n",
    "opt_disc_vq = optim.Adam(\n",
    "    discriminator_vq.parameters(),\n",
    "    lr=CONFIG['lr'], betas=(0.5, 0.9)\n",
    ")\n",
    "\n",
    "# ë°ì´í„° ë¡œë”\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG['batch_size_vqgan'],\n",
    "    shuffle=True,\n",
    "    num_workers=CONFIG['num_workers']\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=CONFIG['batch_size_vqgan'],\n",
    "    shuffle=False,\n",
    "    num_workers=CONFIG['num_workers']\n",
    ")\n",
    "\n",
    "perceptual_loss = SimpleLPIPS().to(CONFIG['device'])\n",
    "\n",
    "# í•™ìŠµ íˆìŠ¤í† ë¦¬\n",
    "vqgan_history = {\n",
    "    'train_loss_gen': [],\n",
    "    'train_loss_disc': [],\n",
    "    'train_recon_loss': [],\n",
    "    'train_vq_loss': [],\n",
    "    'val_loss': [],\n",
    "    'val_psnr': [],\n",
    "    'val_ssim': []\n",
    "}\n",
    "\n",
    "global_step = 0\n",
    "\n",
    "# í•™ìŠµ ë£¨í”„\n",
    "for epoch in range(CONFIG['num_epochs']):\n",
    "    vqgan.train()\n",
    "    discriminator_vq.train()\n",
    "    \n",
    "    epoch_metrics = {'loss_gen': 0, 'loss_disc': 0, 'recon_loss': 0, 'vq_loss': 0}\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"VQ-GAN Epoch {epoch+1}/{CONFIG['num_epochs']}\")\n",
    "    \n",
    "    for batch in pbar:\n",
    "        images = batch[0].to(CONFIG['device'])\n",
    "        \n",
    "        # Train Generator\n",
    "        opt_vq.zero_grad()\n",
    "        recon, vq_loss, _ = vqgan(images)\n",
    "        \n",
    "        recon_loss = torch.abs(images - recon).mean()\n",
    "        p_loss = perceptual_loss(images, recon)\n",
    "        \n",
    "        if global_step > CONFIG['disc_start']:\n",
    "            logits_fake = discriminator_vq(recon)\n",
    "            g_loss = -torch.mean(logits_fake)\n",
    "            disc_factor = CONFIG['disc_weight']\n",
    "        else:\n",
    "            g_loss = torch.tensor(0.0, device=CONFIG['device'])\n",
    "            disc_factor = 0.0\n",
    "        \n",
    "        loss_gen = recon_loss + CONFIG['perceptual_weight'] * p_loss + disc_factor * g_loss + vq_loss\n",
    "        loss_gen.backward()\n",
    "        opt_vq.step()\n",
    "        \n",
    "        # Train Discriminator\n",
    "        if global_step > CONFIG['disc_start']:\n",
    "            opt_disc_vq.zero_grad()\n",
    "            logits_real = discriminator_vq(images.detach())\n",
    "            logits_fake = discriminator_vq(recon.detach())\n",
    "            disc_loss = hinge_d_loss(logits_real, logits_fake)\n",
    "            disc_loss.backward()\n",
    "            opt_disc_vq.step()\n",
    "        else:\n",
    "            disc_loss = torch.tensor(0.0)\n",
    "        \n",
    "        epoch_metrics['loss_gen'] += loss_gen.item()\n",
    "        epoch_metrics['loss_disc'] += disc_loss.item()\n",
    "        epoch_metrics['recon_loss'] += recon_loss.item()\n",
    "        epoch_metrics['vq_loss'] += vq_loss.item()\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'gen': f\"{loss_gen.item():.4f}\",\n",
    "            'disc': f\"{disc_loss.item():.4f}\"\n",
    "        })\n",
    "        \n",
    "        global_step += 1\n",
    "    \n",
    "    # Epoch í‰ê· \n",
    "    for key in epoch_metrics:\n",
    "        epoch_metrics[key] /= len(train_loader)\n",
    "    \n",
    "    vqgan_history['train_loss_gen'].append(epoch_metrics['loss_gen'])\n",
    "    vqgan_history['train_loss_disc'].append(epoch_metrics['loss_disc'])\n",
    "    vqgan_history['train_recon_loss'].append(epoch_metrics['recon_loss'])\n",
    "    vqgan_history['train_vq_loss'].append(epoch_metrics['vq_loss'])\n",
    "    \n",
    "    # Validation\n",
    "    vqgan.eval()\n",
    "    val_loss = 0\n",
    "    val_psnr = 0\n",
    "    val_ssim = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(val_loader):\n",
    "            if i >= 50:  # ë¹ ë¥¸ ê²€ì¦\n",
    "                break\n",
    "            images = batch[0].to(CONFIG['device'])\n",
    "            recon, vq_loss, _ = vqgan(images)\n",
    "            \n",
    "            val_loss += (torch.abs(images - recon).mean() + vq_loss).item()\n",
    "            val_psnr += calculate_psnr(images, recon).item()\n",
    "            val_ssim += calculate_ssim(images, recon).item()\n",
    "    \n",
    "    val_loss /= min(50, len(val_loader))\n",
    "    val_psnr /= min(50, len(val_loader))\n",
    "    val_ssim /= min(50, len(val_loader))\n",
    "    \n",
    "    vqgan_history['val_loss'].append(val_loss)\n",
    "    vqgan_history['val_psnr'].append(val_psnr)\n",
    "    vqgan_history['val_ssim'].append(val_ssim)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1}: Val Loss={val_loss:.4f}, PSNR={val_psnr:.2f}, SSIM={val_ssim:.4f}\\n\")\n",
    "    \n",
    "    # ì¬êµ¬ì„± ìƒ˜í”Œ ë³´ê¸°\n",
    "    if (epoch + 1) % CONFIG['save_interval'] == 0:\n",
    "        with torch.no_grad():\n",
    "            sample_images = next(iter(val_loader))[0][:8].to(CONFIG['device'])\n",
    "            sample_recon, _, _ = vqgan(sample_images)\n",
    "            show_reconstruction(sample_images, sample_recon)\n",
    "\n",
    "# ì²´í¬í¬ì¸íŠ¸ ì €ì¥\n",
    "Path('checkpoints').mkdir(exist_ok=True)\n",
    "torch.save({\n",
    "    'model': vqgan.state_dict(),\n",
    "    'discriminator': discriminator_vq.state_dict(),\n",
    "    'history': vqgan_history\n",
    "}, 'checkpoints/vqgan_trained.pt')\n",
    "\n",
    "print(\"\\nâœ“ VQ-GAN í•™ìŠµ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Efficient-VQGAN í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Efficient-VQGAN í•™ìŠµ ì‹œì‘\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# ëª¨ë¸ ì´ˆê¸°í™”\n",
    "efficient_vqgan = EfficientVQGAN(\n",
    "    in_channels=3,\n",
    "    hidden_dims=[64, 128, 256],\n",
    "    latent_dim=256,\n",
    "    num_codebooks=4,\n",
    "    codebook_size=256\n",
    ").to(CONFIG['device'])\n",
    "\n",
    "discriminator_eff = MultiScaleDiscriminator(in_channels=3, num_scales=3).to(CONFIG['device'])\n",
    "\n",
    "print(f\"Efficient-VQGAN íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in efficient_vqgan.parameters())/1e6:.2f}M\")\n",
    "\n",
    "# ì˜µí‹°ë§ˆì´ì €\n",
    "opt_eff = optim.Adam(\n",
    "    list(efficient_vqgan.encoder.parameters()) + \n",
    "    list(efficient_vqgan.decoder.parameters()) + \n",
    "    list(efficient_vqgan.quantizer.parameters()),\n",
    "    lr=CONFIG['lr'], betas=(0.5, 0.9)\n",
    ")\n",
    "\n",
    "opt_disc_eff = optim.Adam(\n",
    "    discriminator_eff.parameters(),\n",
    "    lr=CONFIG['lr'], betas=(0.5, 0.9)\n",
    ")\n",
    "\n",
    "# ë°ì´í„° ë¡œë”\n",
    "train_loader_eff = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG['batch_size_efficient'],\n",
    "    shuffle=True,\n",
    "    num_workers=CONFIG['num_workers']\n",
    ")\n",
    "\n",
    "val_loader_eff = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=CONFIG['batch_size_efficient'],\n",
    "    shuffle=False,\n",
    "    num_workers=CONFIG['num_workers']\n",
    ")\n",
    "\n",
    "# í•™ìŠµ íˆìŠ¤í† ë¦¬\n",
    "eff_history = {\n",
    "    'train_loss_ae': [],\n",
    "    'train_loss_disc': [],\n",
    "    'train_recon_loss': [],\n",
    "    'train_vq_loss': [],\n",
    "    'val_loss': [],\n",
    "    'val_psnr': [],\n",
    "    'val_ssim': []\n",
    "}\n",
    "\n",
    "global_step = 0\n",
    "\n",
    "# í•™ìŠµ ë£¨í”„\n",
    "for epoch in range(CONFIG['num_epochs']):\n",
    "    efficient_vqgan.train()\n",
    "    discriminator_eff.train()\n",
    "    \n",
    "    epoch_metrics = {'loss_ae': 0, 'loss_disc': 0, 'recon_loss': 0, 'vq_loss': 0}\n",
    "    \n",
    "    pbar = tqdm(train_loader_eff, desc=f\"Eff-VQGAN Epoch {epoch+1}/{CONFIG['num_epochs']}\")\n",
    "    \n",
    "    for batch in pbar:\n",
    "        images = batch[0].to(CONFIG['device'])\n",
    "        \n",
    "        # Train Generator\n",
    "        opt_eff.zero_grad()\n",
    "        recon, vq_loss, _ = efficient_vqgan(images)\n",
    "        \n",
    "        recon_loss = torch.abs(images - recon).mean()\n",
    "        p_loss = perceptual_loss(images, recon)\n",
    "        \n",
    "        if global_step > CONFIG['disc_start']:\n",
    "            logits_fake = discriminator_eff(recon)\n",
    "            g_loss = -sum([torch.mean(logit) for logit in logits_fake]) / len(logits_fake)\n",
    "            disc_factor = CONFIG['disc_weight']\n",
    "        else:\n",
    "            g_loss = torch.tensor(0.0, device=CONFIG['device'])\n",
    "            disc_factor = 0.0\n",
    "        \n",
    "        loss_ae = recon_loss + CONFIG['perceptual_weight'] * p_loss + disc_factor * g_loss + vq_loss\n",
    "        loss_ae.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(efficient_vqgan.parameters(), max_norm=1.0)\n",
    "        opt_eff.step()\n",
    "        \n",
    "        # Train Discriminator\n",
    "        if global_step > CONFIG['disc_start']:\n",
    "            opt_disc_eff.zero_grad()\n",
    "            logits_real = discriminator_eff(images.detach())\n",
    "            logits_fake = discriminator_eff(recon.detach())\n",
    "            \n",
    "            disc_loss = 0\n",
    "            for lr, lf in zip(logits_real, logits_fake):\n",
    "                disc_loss += hinge_d_loss(lr, lf)\n",
    "            disc_loss = disc_loss / len(logits_real)\n",
    "            \n",
    "            disc_loss.backward()\n",
    "            opt_disc_eff.step()\n",
    "        else:\n",
    "            disc_loss = torch.tensor(0.0)\n",
    "        \n",
    "        epoch_metrics['loss_ae'] += loss_ae.item()\n",
    "        epoch_metrics['loss_disc'] += disc_loss.item()\n",
    "        epoch_metrics['recon_loss'] += recon_loss.item()\n",
    "        epoch_metrics['vq_loss'] += vq_loss.item()\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'ae': f\"{loss_ae.item():.4f}\",\n",
    "            'disc': f\"{disc_loss.item():.4f}\"\n",
    "        })\n",
    "        \n",
    "        global_step += 1\n",
    "    \n",
    "    # Epoch í‰ê· \n",
    "    for key in epoch_metrics:\n",
    "        epoch_metrics[key] /= len(train_loader_eff)\n",
    "    \n",
    "    eff_history['train_loss_ae'].append(epoch_metrics['loss_ae'])\n",
    "    eff_history['train_loss_disc'].append(epoch_metrics['loss_disc'])\n",
    "    eff_history['train_recon_loss'].append(epoch_metrics['recon_loss'])\n",
    "    eff_history['train_vq_loss'].append(epoch_metrics['vq_loss'])\n",
    "    \n",
    "    # Validation\n",
    "    efficient_vqgan.eval()\n",
    "    val_loss = 0\n",
    "    val_psnr = 0\n",
    "    val_ssim = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(val_loader_eff):\n",
    "            if i >= 50:\n",
    "                break\n",
    "            images = batch[0].to(CONFIG['device'])\n",
    "            recon, vq_loss, _ = efficient_vqgan(images)\n",
    "            \n",
    "            val_loss += (torch.abs(images - recon).mean() + vq_loss).item()\n",
    "            val_psnr += calculate_psnr(images, recon).item()\n",
    "            val_ssim += calculate_ssim(images, recon).item()\n",
    "    \n",
    "    val_loss /= min(50, len(val_loader_eff))\n",
    "    val_psnr /= min(50, len(val_loader_eff))\n",
    "    val_ssim /= min(50, len(val_loader_eff))\n",
    "    \n",
    "    eff_history['val_loss'].append(val_loss)\n",
    "    eff_history['val_psnr'].append(val_psnr)\n",
    "    eff_history['val_ssim'].append(val_ssim)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1}: Val Loss={val_loss:.4f}, PSNR={val_psnr:.2f}, SSIM={val_ssim:.4f}\\n\")\n",
    "    \n",
    "    # ì¬êµ¬ì„± ìƒ˜í”Œ ë³´ê¸°\n",
    "    if (epoch + 1) % CONFIG['save_interval'] == 0:\n",
    "        with torch.no_grad():\n",
    "            sample_images = next(iter(val_loader_eff))[0][:8].to(CONFIG['device'])\n",
    "            sample_recon, _, _ = efficient_vqgan(sample_images)\n",
    "            show_reconstruction(sample_images, sample_recon)\n",
    "\n",
    "# ì²´í¬í¬ì¸íŠ¸ ì €ì¥\n",
    "torch.save({\n",
    "    'model': efficient_vqgan.state_dict(),\n",
    "    'discriminator': discriminator_eff.state_dict(),\n",
    "    'history': eff_history\n",
    "}, 'checkpoints/efficient_vqgan_trained.pt')\n",
    "\n",
    "print(\"\\nâœ“ Efficient-VQGAN í•™ìŠµ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. í•™ìŠµ ê³¡ì„  ë¹„êµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµ ê³¡ì„  ì‹œê°í™”\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "epochs = list(range(1, CONFIG['num_epochs'] + 1))\n",
    "\n",
    "# Loss comparison\n",
    "axes[0, 0].plot(epochs, vqgan_history['train_loss_gen'], 'o-', label='VQ-GAN', linewidth=2)\n",
    "axes[0, 0].plot(epochs, eff_history['train_loss_ae'], 's-', label='Efficient-VQGAN', linewidth=2)\n",
    "axes[0, 0].set_title('Generator/Autoencoder Loss', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Discriminator loss\n",
    "axes[0, 1].plot(epochs, vqgan_history['train_loss_disc'], 'o-', label='VQ-GAN', linewidth=2)\n",
    "axes[0, 1].plot(epochs, eff_history['train_loss_disc'], 's-', label='Efficient-VQGAN', linewidth=2)\n",
    "axes[0, 1].set_title('Discriminator Loss', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Reconstruction loss\n",
    "axes[0, 2].plot(epochs, vqgan_history['train_recon_loss'], 'o-', label='VQ-GAN', linewidth=2)\n",
    "axes[0, 2].plot(epochs, eff_history['train_recon_loss'], 's-', label='Efficient-VQGAN', linewidth=2)\n",
    "axes[0, 2].set_title('Reconstruction Loss', fontsize=14, fontweight='bold')\n",
    "axes[0, 2].set_xlabel('Epoch')\n",
    "axes[0, 2].set_ylabel('Loss')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Val loss\n",
    "axes[1, 0].plot(epochs, vqgan_history['val_loss'], 'o-', label='VQ-GAN', linewidth=2)\n",
    "axes[1, 0].plot(epochs, eff_history['val_loss'], 's-', label='Efficient-VQGAN', linewidth=2)\n",
    "axes[1, 0].set_title('Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Loss')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# PSNR\n",
    "axes[1, 1].plot(epochs, vqgan_history['val_psnr'], 'o-', label='VQ-GAN', linewidth=2)\n",
    "axes[1, 1].plot(epochs, eff_history['val_psnr'], 's-', label='Efficient-VQGAN', linewidth=2)\n",
    "axes[1, 1].set_title('Validation PSNR', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('PSNR (dB)')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# SSIM\n",
    "axes[1, 2].plot(epochs, vqgan_history['val_ssim'], 'o-', label='VQ-GAN', linewidth=2)\n",
    "axes[1, 2].plot(epochs, eff_history['val_ssim'], 's-', label='Efficient-VQGAN', linewidth=2)\n",
    "axes[1, 2].set_title('Validation SSIM', fontsize=14, fontweight='bold')\n",
    "axes[1, 2].set_xlabel('Epoch')\n",
    "axes[1, 2].set_ylabel('SSIM')\n",
    "axes[1, 2].legend()\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ í•™ìŠµ ê³¡ì„  ì €ì¥: training_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# ì¶”ë¡  ì†ë„ ì¸¡ì •\n",
    "def measure_inference_time(model, input_size=(1, 3, 256, 256), num_runs=100):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    dummy = torch.randn(input_size).to(device)\n",
    "    \n",
    "    # Warmup\n",
    "    with torch.no_grad():\n",
    "        for _ in range(10):\n",
    "            _ = model(dummy)\n",
    "    \n",
    "    # Measure\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    times = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_runs):\n",
    "            start = time.time()\n",
    "            _ = model(dummy)\n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "            times.append(time.time() - start)\n",
    "    \n",
    "    return np.mean(times) * 1000, np.std(times) * 1000\n",
    "\n",
    "# ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰\n",
    "results = {}\n",
    "\n",
    "# VQ-GAN\n",
    "print(\"VQ-GAN ë²¤ì¹˜ë§ˆí¬...\")\n",
    "inf_time, inf_std = measure_inference_time(vqgan)\n",
    "results['VQ-GAN'] = {\n",
    "    'params_M': sum(p.numel() for p in vqgan.parameters()) / 1e6,\n",
    "    'inference_ms': inf_time,\n",
    "    'inference_std': inf_std,\n",
    "    'fps': 1000 / inf_time,\n",
    "    'final_val_loss': vqgan_history['val_loss'][-1],\n",
    "    'final_psnr': vqgan_history['val_psnr'][-1],\n",
    "    'final_ssim': vqgan_history['val_ssim'][-1]\n",
    "}\n",
    "\n",
    "# Efficient-VQGAN\n",
    "print(\"Efficient-VQGAN ë²¤ì¹˜ë§ˆí¬...\")\n",
    "inf_time, inf_std = measure_inference_time(efficient_vqgan)\n",
    "results['Efficient-VQGAN'] = {\n",
    "    'params_M': sum(p.numel() for p in efficient_vqgan.parameters()) / 1e6,\n",
    "    'inference_ms': inf_time,\n",
    "    'inference_std': inf_std,\n",
    "    'fps': 1000 / inf_time,\n",
    "    'final_val_loss': eff_history['val_loss'][-1],\n",
    "    'final_psnr': eff_history['val_psnr'][-1],\n",
    "    'final_ssim': eff_history['val_ssim'][-1]\n",
    "}\n",
    "\n",
    "# ê²°ê³¼ í…Œì´ë¸”\n",
    "df_results = pd.DataFrame(results).T\n",
    "df_results.columns = ['Parameters (M)', 'Inference (ms)', 'Std (ms)', 'FPS', 'Val Loss', 'PSNR (dB)', 'SSIM']\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼\")\n",
    "print(\"=\"*80)\n",
    "print(df_results.to_string())\n",
    "print(\"=\"*80)\n",
    "\n",
    "# CSV ì €ì¥\n",
    "df_results.to_csv('benchmark_results.csv')\n",
    "print(\"\\nâœ“ ê²°ê³¼ ì €ì¥: benchmark_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ìµœì¢… ë¹„êµ ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¢…í•© ë¹„êµ ì°¨íŠ¸\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "models = ['VQ-GAN', 'Efficient-VQGAN']\n",
    "colors = ['#3498db', '#2ecc71']\n",
    "\n",
    "# 1. Parameters\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "params = [results[m]['params_M'] for m in models]\n",
    "bars = ax1.bar(models, params, color=colors)\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.2f}M', ha='center', va='bottom', fontweight='bold')\n",
    "ax1.set_title('Model Size', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Parameters (M)')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 2. Inference Time\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "inf_time = [results[m]['inference_ms'] for m in models]\n",
    "bars = ax2.bar(models, inf_time, color=colors)\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.2f}ms', ha='center', va='bottom', fontweight='bold')\n",
    "ax2.set_title('Inference Time', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Time (ms)')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. FPS\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "fps = [results[m]['fps'] for m in models]\n",
    "bars = ax3.bar(models, fps, color=colors)\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "ax3.set_title('Throughput', fontsize=12, fontweight='bold')\n",
    "ax3.set_ylabel('FPS')\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 4. PSNR\n",
    "ax4 = fig.add_subplot(gs[1, 0])\n",
    "psnr = [results[m]['final_psnr'] for m in models]\n",
    "bars = ax4.bar(models, psnr, color=colors)\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "ax4.set_title('PSNR (Higher is Better)', fontsize=12, fontweight='bold')\n",
    "ax4.set_ylabel('PSNR (dB)')\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 5. SSIM\n",
    "ax5 = fig.add_subplot(gs[1, 1])\n",
    "ssim = [results[m]['final_ssim'] for m in models]\n",
    "bars = ax5.bar(models, ssim, color=colors)\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax5.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "ax5.set_title('SSIM (Higher is Better)', fontsize=12, fontweight='bold')\n",
    "ax5.set_ylabel('SSIM')\n",
    "ax5.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 6. Val Loss\n",
    "ax6 = fig.add_subplot(gs[1, 2])\n",
    "val_loss = [results[m]['final_val_loss'] for m in models]\n",
    "bars = ax6.bar(models, val_loss, color=colors)\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax6.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "ax6.set_title('Validation Loss (Lower is Better)', fontsize=12, fontweight='bold')\n",
    "ax6.set_ylabel('Loss')\n",
    "ax6.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 7. Radar Chart\n",
    "ax7 = fig.add_subplot(gs[2, :], projection='polar')\n",
    "\n",
    "# Normalize metrics\n",
    "categories = ['Speed\\n(FPS)', 'Quality\\n(PSNR)', 'Quality\\n(SSIM)', 'Efficiency\\n(Params)', 'Loss']\n",
    "\n",
    "def normalize(values, inverse=False):\n",
    "    vmin, vmax = min(values), max(values)\n",
    "    if vmax == vmin:\n",
    "        return [0.5] * len(values)\n",
    "    if inverse:\n",
    "        return [(vmax - v) / (vmax - vmin) for v in values]\n",
    "    return [(v - vmin) / (vmax - vmin) for v in values]\n",
    "\n",
    "vqgan_values = [\n",
    "    normalize([results['VQ-GAN']['fps'], results['Efficient-VQGAN']['fps']])[0],\n",
    "    normalize([results['VQ-GAN']['final_psnr'], results['Efficient-VQGAN']['final_psnr']])[0],\n",
    "    normalize([results['VQ-GAN']['final_ssim'], results['Efficient-VQGAN']['final_ssim']])[0],\n",
    "    normalize([results['VQ-GAN']['params_M'], results['Efficient-VQGAN']['params_M']], inverse=True)[0],\n",
    "    normalize([results['VQ-GAN']['final_val_loss'], results['Efficient-VQGAN']['final_val_loss']], inverse=True)[0]\n",
    "]\n",
    "\n",
    "eff_values = [\n",
    "    normalize([results['VQ-GAN']['fps'], results['Efficient-VQGAN']['fps']])[1],\n",
    "    normalize([results['VQ-GAN']['final_psnr'], results['Efficient-VQGAN']['final_psnr']])[1],\n",
    "    normalize([results['VQ-GAN']['final_ssim'], results['Efficient-VQGAN']['final_ssim']])[1],\n",
    "    normalize([results['VQ-GAN']['params_M'], results['Efficient-VQGAN']['params_M']], inverse=True)[1],\n",
    "    normalize([results['VQ-GAN']['final_val_loss'], results['Efficient-VQGAN']['final_val_loss']], inverse=True)[1]\n",
    "]\n",
    "\n",
    "angles = [n / float(len(categories)) * 2 * np.pi for n in range(len(categories))]\n",
    "vqgan_values += vqgan_values[:1]\n",
    "eff_values += eff_values[:1]\n",
    "angles += angles[:1]\n",
    "\n",
    "ax7.plot(angles, vqgan_values, 'o-', linewidth=2, label='VQ-GAN', color=colors[0])\n",
    "ax7.fill(angles, vqgan_values, alpha=0.15, color=colors[0])\n",
    "ax7.plot(angles, eff_values, 's-', linewidth=2, label='Efficient-VQGAN', color=colors[1])\n",
    "ax7.fill(angles, eff_values, alpha=0.15, color=colors[1])\n",
    "\n",
    "ax7.set_xticks(angles[:-1])\n",
    "ax7.set_xticklabels(categories, size=11)\n",
    "ax7.set_ylim(0, 1)\n",
    "ax7.set_title('Overall Performance Comparison (Normalized)', fontsize=14, fontweight='bold', pad=20)\n",
    "ax7.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=11)\n",
    "ax7.grid(True)\n",
    "\n",
    "plt.savefig('final_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ ìµœì¢… ë¹„êµ ì°¨íŠ¸ ì €ì¥: final_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML ë¦¬í¬íŠ¸ ìƒì„±\n",
    "report_html = f\"\"\"\n",
    "<html>\n",
    "<head>\n",
    "    <title>VQ-GAN vs Efficient-VQGAN ë¹„êµ ë¦¬í¬íŠ¸</title>\n",
    "    <style>\n",
    "        body {{ font-family: Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }}\n",
    "        h1 {{ color: #2c3e50; border-bottom: 3px solid #3498db; padding-bottom: 10px; }}\n",
    "        h2 {{ color: #34495e; margin-top: 30px; }}\n",
    "        table {{ border-collapse: collapse; width: 100%; margin: 20px 0; background: white; box-shadow: 0 2px 3px rgba(0,0,0,0.1); }}\n",
    "        th, td {{ border: 1px solid #ddd; padding: 12px; text-align: left; }}\n",
    "        th {{ background-color: #3498db; color: white; }}\n",
    "        tr:nth-child(even) {{ background-color: #f8f9fa; }}\n",
    "        .winner {{ background-color: #d4edda; font-weight: bold; }}\n",
    "        .metric {{ font-size: 24px; font-weight: bold; color: #2c3e50; }}\n",
    "        .summary {{ background: white; padding: 20px; margin: 20px 0; border-left: 4px solid #3498db; }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>ğŸ”¥ VQ-GAN vs Efficient-VQGAN ë¹„êµ ë¦¬í¬íŠ¸</h1>\n",
    "    \n",
    "    <div class=\"summary\">\n",
    "        <h2>ğŸ“Š Executive Summary</h2>\n",
    "        <p><strong>í•™ìŠµ ì„¤ì •:</strong> {CONFIG['num_epochs']} epochs, Dataset: {CONFIG['dataset']}</p>\n",
    "        <p><strong>íŒŒë¼ë¯¸í„° ê°ì†Œ:</strong> {(1 - results['Efficient-VQGAN']['params_M']/results['VQ-GAN']['params_M'])*100:.1f}%</p>\n",
    "        <p><strong>ì†ë„ í–¥ìƒ:</strong> {(results['Efficient-VQGAN']['fps']/results['VQ-GAN']['fps']-1)*100:.1f}%</p>\n",
    "    </div>\n",
    "    \n",
    "    <h2>ğŸ“ˆ ì„±ëŠ¥ ë¹„êµ í…Œì´ë¸”</h2>\n",
    "    {df_results.to_html(classes='table')}\n",
    "    \n",
    "    <h2>ğŸ† ì£¼ìš” ë°œê²¬ì‚¬í•­</h2>\n",
    "    <div class=\"summary\">\n",
    "        <ul>\n",
    "            <li><strong>ëª¨ë¸ í¬ê¸°:</strong> Efficient-VQGANì´ {(1 - results['Efficient-VQGAN']['params_M']/results['VQ-GAN']['params_M'])*100:.1f}% ë” ì‘ìŒ</li>\n",
    "            <li><strong>ì¶”ë¡  ì†ë„:</strong> Efficient-VQGANì´ {'ë” ë¹ ë¦„' if results['Efficient-VQGAN']['fps'] > results['VQ-GAN']['fps'] else 'ë” ëŠë¦¼'} ({abs((results['Efficient-VQGAN']['fps']/results['VQ-GAN']['fps']-1)*100):.1f}%)</li>\n",
    "            <li><strong>ì´ë¯¸ì§€ í’ˆì§ˆ (PSNR):</strong> {'VQ-GAN' if results['VQ-GAN']['final_psnr'] > results['Efficient-VQGAN']['final_psnr'] else 'Efficient-VQGAN'}ì´ {abs(results['VQ-GAN']['final_psnr'] - results['Efficient-VQGAN']['final_psnr']):.2f} dB ìš°ì„¸</li>\n",
    "            <li><strong>ì „ì²´ í‰ê°€:</strong> Efficient-VQGANì€ íš¨ìœ¨ì„±ê³¼ í’ˆì§ˆì˜ ì¢‹ì€ ê· í˜•ì„ ì œê³µ</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "    \n",
    "    <h2>ğŸ“… ìƒì„± ì •ë³´</h2>\n",
    "    <p>ìƒì„± ì¼ì‹œ: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}</p>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "with open('comparison_report.html', 'w', encoding='utf-8') as f:\n",
    "    f.write(report_html)\n",
    "\n",
    "print(\"âœ“ HTML ë¦¬í¬íŠ¸ ì €ì¥: comparison_report.html\")\n",
    "\n",
    "# ë…¸íŠ¸ë¶ì—ì„œ ë¦¬í¬íŠ¸ í‘œì‹œ\n",
    "display(HTML(report_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. ì™„ë£Œ ìš”ì•½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ‰ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nìƒì„±ëœ íŒŒì¼:\")\n",
    "print(\"  âœ“ checkpoints/vqgan_trained.pt          - VQ-GAN ì²´í¬í¬ì¸íŠ¸\")\n",
    "print(\"  âœ“ checkpoints/efficient_vqgan_trained.pt - Efficient-VQGAN ì²´í¬í¬ì¸íŠ¸\")\n",
    "print(\"  âœ“ training_comparison.png                - í•™ìŠµ ê³¡ì„  ë¹„êµ\")\n",
    "print(\"  âœ“ final_comparison.png                   - ìµœì¢… ì„±ëŠ¥ ë¹„êµ\")\n",
    "print(\"  âœ“ benchmark_results.csv                  - ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼\")\n",
    "print(\"  âœ“ comparison_report.html                 - HTML ë¦¬í¬íŠ¸\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nì£¼ìš” ê²°ê³¼:\")\n",
    "print(f\"  â€¢ VQ-GAN íŒŒë¼ë¯¸í„°:          {results['VQ-GAN']['params_M']:.2f}M\")\n",
    "print(f\"  â€¢ Efficient-VQGAN íŒŒë¼ë¯¸í„°: {results['Efficient-VQGAN']['params_M']:.2f}M\")\n",
    "print(f\"  â€¢ íŒŒë¼ë¯¸í„° ê°ì†Œ:            {(1 - results['Efficient-VQGAN']['params_M']/results['VQ-GAN']['params_M'])*100:.1f}%\")\n",
    "print(f\"  â€¢ VQ-GAN FPS:               {results['VQ-GAN']['fps']:.2f}\")\n",
    "print(f\"  â€¢ Efficient-VQGAN FPS:      {results['Efficient-VQGAN']['fps']:.2f}\")\n",
    "print(f\"  â€¢ ì†ë„ ì°¨ì´:                {abs((results['Efficient-VQGAN']['fps']/results['VQ-GAN']['fps']-1)*100):.1f}%\")\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
